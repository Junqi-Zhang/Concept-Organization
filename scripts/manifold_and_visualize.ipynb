{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "IMAGE_DATA_FILE = \"../data/caltech-256_features.npz\"\n",
    "CLASS_NAME_FILE = \"../data/256_ObjectCategories_map_ZH.csv\"\n",
    "\n",
    "image_data = np.load(IMAGE_DATA_FILE)\n",
    "# print(f\"Components of image_data: {list(image_data.keys())}\")\n",
    "\n",
    "X_vit = image_data.get(\"vit_features\")\n",
    "print(f\"The shape of X_vit is {X_vit.shape}\")\n",
    "\n",
    "X_clip = image_data.get(\"clip_features\")\n",
    "print(f\"The shape of X_clip is {X_clip.shape}\")\n",
    "\n",
    "y = image_data.get(\"labels\")\n",
    "print(f\"The shape of y is {y.shape}\")\n",
    "\n",
    "class_name_df = pd.read_csv(CLASS_NAME_FILE)\n",
    "class_name_map = class_name_df.set_index(\"class\")[\"handle\"].to_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 数据标准化，分别针对 ViT 和 Clip 两模型的输出特征，在待分析(聚类或可视化)样本集合上，尝试多种强度（对原始特征的影响）不同的标准化手段：\n",
    "    - 无标准化。\n",
    "    - 样本点 L2 范数均值的单位化，即仅放缩两个模型的输出，使全部待分析样本点分布在单位球面附近。\n",
    "    - 各特征的 Standard Scaler, 使全部待分析样本点分布呈近似单位立方体分布, scipy.cluster.vq.whiten or sklearn.preprocessing.StandardScaler。\n",
    "    - PCA/ZCA Whitening。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from standardization import get_standard_data\n",
    "\n",
    "# STANDARD_METHOD = None\n",
    "# STANDARD_METHOD = \"l2_norm\"\n",
    "STANDARD_METHOD = \"feature_standard\"\n",
    "# STANDARD_METHOD = \"PCA_whiten\"\n",
    "# STANDARD_METHOD = \"ZCA_whiten\"\n",
    "\n",
    "X_vit = get_standard_data(X_vit, STANDARD_METHOD)\n",
    "X_clip = get_standard_data(X_clip, STANDARD_METHOD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获取各类别的表示\n",
    "    - 在数据中剔除“其它”\n",
    "    - 各类别全部样本点的重心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(y)\n",
    "# 最后一类是其它，后续分析应剔除\n",
    "print(f\"The last categoty is \\\"{labels[-1]}\\\".\")\n",
    "not_clutter_index = np.where(y != labels[-1])\n",
    "X_vit = X_vit[not_clutter_index]\n",
    "X_clip = X_clip[not_clutter_index]\n",
    "y = y[not_clutter_index]\n",
    "print(f\"Samples of Category \\\"{labels[-1]}\\\" are removed.\")\n",
    "\n",
    "X_vit_class = np.array([np.mean(X_vit[np.where(y == label)], axis=0)\n",
    "                        for label in labels[:-1]])\n",
    "X_clip_class = np.array([np.mean(X_clip[np.where(y == label)], axis=0)\n",
    "                         for label in labels[:-1]])\n",
    "y_class = np.array([label for label in labels[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# FIT_LEVEL = \"image\"\n",
    "FIT_LEVEL = \"class\"\n",
    "\n",
    "# METRIC = \"euclidean\"\n",
    "METRIC = \"cityblock\"\n",
    "\n",
    "N_ITER = 10000\n",
    "VERBOSE = 1\n",
    "N_JOBS = 12\n",
    "\n",
    "if FIT_LEVEL == \"image\":\n",
    "\n",
    "    PERPLEXITY = 30\n",
    "    transformer = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=PERPLEXITY,\n",
    "        early_exaggeration=12,\n",
    "        learning_rate=\"auto\",\n",
    "        n_iter=N_ITER,\n",
    "        n_iter_without_progress=300,\n",
    "        metric=METRIC,\n",
    "        init=\"pca\",\n",
    "        verbose=VERBOSE,\n",
    "        random_state=0,\n",
    "        method=\"barnes_hut\",\n",
    "        angle=0.5,\n",
    "        n_jobs=N_JOBS\n",
    "    )\n",
    "\n",
    "    X_vit_class_embedded = transformer.fit_transform(\n",
    "        np.vstack((X_vit, X_vit_class)))[-len(X_vit_class):]\n",
    "    X_clip_class_embedded = transformer.fit_transform(\n",
    "        np.vstack((X_clip, X_clip_class)))[-len(X_clip_class):]\n",
    "\n",
    "elif FIT_LEVEL == \"class\":\n",
    "\n",
    "    PERPLEXITY = 10\n",
    "    transformer = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=PERPLEXITY,\n",
    "        early_exaggeration=12,\n",
    "        learning_rate=\"auto\",\n",
    "        n_iter=N_ITER,\n",
    "        n_iter_without_progress=300,\n",
    "        metric=METRIC,\n",
    "        init=\"pca\",\n",
    "        verbose=VERBOSE,\n",
    "        random_state=0,\n",
    "        method=\"barnes_hut\",\n",
    "        angle=0.5,\n",
    "        n_jobs=N_JOBS\n",
    "    )\n",
    "\n",
    "    X_vit_class_embedded = transformer.fit_transform(X_vit_class)\n",
    "    X_clip_class_embedded = transformer.fit_transform(X_clip_class)\n",
    "\n",
    "else:\n",
    "    ValueError(\n",
    "        f\"Target level {FIT_LEVEL} not available, only image or class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "FIGURE_PATH = \"../results/t-SNE/\"\n",
    "FIGURE_FILE = f\"fit_{FIT_LEVEL}_transform_class_&_{STANDARD_METHOD}_preprocess_&_{METRIC}_metric_&_{N_ITER}_iters\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2,\n",
    "                         figsize=(70, 30),\n",
    "                         gridspec_kw={'width_ratios': [1, 1]})\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "def plot_embedding(X, y, ax, title):\n",
    "    # X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    print(f\"There are {len(labels)} classes in {title} data.\")\n",
    "\n",
    "    cmap = colormaps.get_cmap(\"rainbow\")\n",
    "    colors = cmap(np.linspace(0, 1, len(labels)))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(colors)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        ax.scatter(\n",
    "            *X[y == label].T,\n",
    "            color=colors[i]\n",
    "        )\n",
    "        # display(fig)\n",
    "        # clear_output(wait = True)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        y_code, y_en = y[i].split(\".\")\n",
    "        y_zh = class_name_map[y_en]\n",
    "        ax.annotate(y_code+\".\"+y_zh,\n",
    "                    (X[i][0], X[i][1]), fontproperties=\"SimHei\")\n",
    "\n",
    "    ax.set_title(title, fontsize=30)\n",
    "\n",
    "\n",
    "plot_embedding(X_vit_class_embedded, y_class, axes[0], \"vit_class\")\n",
    "plot_embedding(X_clip_class_embedded, y_class, axes[1], \"clip_class\")\n",
    "\n",
    "\n",
    "plt.suptitle(FIGURE_FILE, x=0.5, y=0.98, fontsize=40)\n",
    "plt.savefig(FIGURE_PATH+FIGURE_FILE+\".pdf\", format=\"pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
